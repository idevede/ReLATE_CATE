{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, autograd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def write_results_to_file(filename, data):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=2)\n",
    "\n",
    "def load_results(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        dataset= pickle.load(handle)\n",
    "        return dataset\n",
    "\n",
    "def append_results_to_file(filename, data):\n",
    "    with open(filename, 'a+b') as handle:\n",
    "        pickle.dump(data, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, y, t, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        all_treatment = []\n",
    "        for k in range(int(len(data)*0.8)): # maximum length is 20 len(data)\n",
    "            data_window = []\n",
    "            label_window = []\n",
    "            for i in range(T, len(data[k])):\n",
    "                data_window.append(data[k][i-T:i])\n",
    "            all_data.append(data_window)\n",
    "            all_label.append(y[k][T+1:])\n",
    "            all_treatment.append(t[k][T:])\n",
    "        \n",
    "        x = np.array(all_data)\n",
    "        \n",
    "        y = np.array(all_label)\n",
    "        \n",
    "        t = np.array(all_treatment)\n",
    "            \n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        #self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.t = torch.from_numpy(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index], self.t[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_in_test(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, y, t, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        all_treatment = []\n",
    "        for k in range(int(len(data)*0.8),len(data)): # maximum length is 20 len(data)\n",
    "            data_window = []\n",
    "            label_window = []\n",
    "            for i in range(T, len(data[k])):\n",
    "                data_window.append(data[k][i-T:i])\n",
    "            all_data.append(data_window)\n",
    "            all_label.append(y[k][T+1:])\n",
    "            all_treatment.append(t[k][T:])\n",
    "        \n",
    "        x = np.array(all_data)\n",
    "        \n",
    "        y = np.array(all_label)\n",
    "        \n",
    "        t = np.array(all_treatment)\n",
    "            \n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        #self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.t = torch.from_numpy(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index], self.t[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_test(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, data, y, t, T):\n",
    "        \"\"\"Initialization\"\"\" \n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        all_treatment = []\n",
    "        for k in range(len(data)): # maximum length is 20 len(data)\n",
    "            data_window = []\n",
    "            label_window = []\n",
    "            for i in range(T, len(data[k])):\n",
    "                data_window.append(data[k][i-T:i])\n",
    "            all_data.append(data_window)\n",
    "            all_label.append(y[k][T+1:])\n",
    "            all_treatment.append(t[k][T:])\n",
    "        \n",
    "        x = np.array(all_data)\n",
    "        \n",
    "        y = np.array(all_label)\n",
    "        \n",
    "        t = np.array(all_treatment)\n",
    "            \n",
    "        self.length = len(x)\n",
    "\n",
    "        x = torch.from_numpy(x)\n",
    "        #self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.x = x #torch.unsqueeze(x, 1)\n",
    "        self.y = torch.from_numpy(y)\n",
    "        self.t = torch.from_numpy(t)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates samples of data\"\"\"\n",
    "        return self.x[index], self.y[index], self.t[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = load_results('/Covid-dataset/causalcovid19/data_2023/train_envs.txt')\n",
    "test_envs = load_results('/Covid-dataset/causalcovid19/data_2023/test_envs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_loader = []\n",
    "for key in train_envs.keys():\n",
    "    dataset_train = Dataset(train_envs[key]['previous_covariates'], train_envs[key]['outcomes'],train_envs[key]['previous_treatments'], T=5)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)  \n",
    "    all_train_loader.append(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_loader = []\n",
    "for key in test_envs.keys():\n",
    "    dataset_train = Dataset_test(test_envs[key]['previous_covariates'], test_envs[key]['outcomes'],test_envs[key]['previous_treatments'], T=5)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)  \n",
    "    all_test_loader.append(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_in_loader = []\n",
    "for key in train_envs.keys():\n",
    "    dataset_train = Dataset_in_test(train_envs[key]['previous_covariates'], train_envs[key]['outcomes'],train_envs[key]['previous_treatments'], T=5)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)  \n",
    "    all_test_in_loader.append(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_res(u_pred,u_label):\n",
    "    \n",
    "    d = u_pred - u_label\n",
    "    mse_f = np.mean(d**2)\n",
    "    mae_f = np.mean(abs(d))\n",
    "    rmse_f = np.sqrt(mse_f)\n",
    "    print(\"Results \")\n",
    "    \n",
    "    print(\"MAE:\", mae_f)\n",
    "    print(\"MSE:\", mse_f)\n",
    "    print(\"RMSE:\", rmse_f)\n",
    "    return mae_f, mse_f, rmse_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n",
    "    \"\"\"Compute the matrix of all squared pairwise distances.\n",
    "    Arguments\n",
    "    ---------\n",
    "    sample_1 : torch.Tensor or Variable\n",
    "        The first sample, should be of shape ``(n_1, d)``.\n",
    "    sample_2 : torch.Tensor or Variable\n",
    "        The second sample, should be of shape ``(n_2, d)``.\n",
    "    norm : float\n",
    "        The l_p norm to be used.\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor or Variable\n",
    "        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to\n",
    "        ``|| sample_1[i, :] - sample_2[j, :] ||_p``.\"\"\"\n",
    "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
    "    norm = float(norm)\n",
    "    if norm == 2.:\n",
    "        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
    "        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
    "        norms = (norms_1.expand(n_1, n_2) +\n",
    "                 norms_2.transpose(0, 1).expand(n_1, n_2))\n",
    "        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
    "        return torch.sqrt(eps + torch.abs(distances_squared))\n",
    "    else:\n",
    "        dim = sample_1.size(1)\n",
    "        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n",
    "        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n",
    "        differences = torch.abs(expanded_1 - expanded_2) ** norm\n",
    "        inner = torch.sum(differences, dim=2, keepdim=False)\n",
    "        return (eps + inner) ** (1. / norm)\n",
    "\n",
    "def wasserstein(x,y,p=0.5,lam=10,its=10,sq=False,backpropT=False,cuda=False):\n",
    "    \"\"\"return W dist between x and y\"\"\"\n",
    "    '''distance matrix M'''\n",
    "    nx = x.shape[0]\n",
    "    ny = y.shape[0]\n",
    "    \n",
    "    x = x.squeeze()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "#    pdist = torch.nn.PairwiseDistance(p=2)\n",
    "\n",
    "    M = pdist(x,y) #distance_matrix(x,y,p=2)\n",
    "    \n",
    "    '''estimate lambda and delta'''\n",
    "    M_mean = torch.mean(M)\n",
    "    M_drop = F.dropout(M,10.0/(nx*ny))\n",
    "    delta = torch.max(M_drop).detach()\n",
    "    eff_lam = (lam/M_mean).detach()\n",
    "\n",
    "    '''compute new distance matrix'''\n",
    "    Mt = M\n",
    "    row = delta*torch.ones(M[0:1,:].shape)\n",
    "    col = torch.cat([delta*torch.ones(M[:,0:1].shape),torch.zeros((1,1))],0)\n",
    "    if cuda:\n",
    "        row = row.cuda()\n",
    "        col = col.cuda()\n",
    "        M = M.cuda()\n",
    "    Mt = torch.cat([M,row],0)\n",
    "    Mt = torch.cat([Mt,col],1)\n",
    "\n",
    "    '''compute marginal'''\n",
    "    a = torch.cat([p*torch.ones((nx,1))/nx,(1-p)*torch.ones((1,1))],0)\n",
    "    b = torch.cat([(1-p)*torch.ones((ny,1))/ny, p*torch.ones((1,1))],0)\n",
    "\n",
    "    '''compute kernel'''\n",
    "    Mlam = eff_lam * Mt\n",
    "    temp_term = torch.ones(1)*1e-6\n",
    "    if cuda:\n",
    "        temp_term = temp_term.cuda()\n",
    "        a = a.cuda()\n",
    "        b = b.cuda()\n",
    "    K = torch.exp(-Mlam) + temp_term\n",
    "    U = K * Mt\n",
    "    ainvK = K/a\n",
    "\n",
    "    u = a\n",
    "\n",
    "    for i in range(its):\n",
    "        u = 1.0/(ainvK.matmul(b/torch.t(torch.t(u).matmul(K))))\n",
    "        if cuda:\n",
    "            u = u.cuda()\n",
    "    v = b/(torch.t(torch.t(u).matmul(K)))\n",
    "    if cuda:\n",
    "        v = v.cuda()\n",
    "\n",
    "    upper_t = u*(torch.t(v)*K).detach()\n",
    "\n",
    "    E = upper_t*Mt\n",
    "    D = 2*torch.sum(E)\n",
    "\n",
    "    if cuda:\n",
    "        D = D.cuda()\n",
    "\n",
    "    return D, Mlam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 'mine' #'wasserstein' #'mine'\n",
    "net = 'tarnet'\n",
    "gpu = 1\n",
    "# # Define and instantiate the model\n",
    " # # Define and instantiate the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        #hidden_dim = flags.hidden_dim\n",
    "        hypo_dim = 100\n",
    "        self.lin1 = nn.Linear(dim, hidden_dim)\n",
    "        self.lin1_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin1_2 = nn.Linear(hidden_dim, hypo_dim)\n",
    "        # tarnet\n",
    "        if net == 'tarnet':\n",
    "            self.lin1_3 = nn.Linear(dim, 1)\n",
    "        else:\n",
    "            self.lin1_3 = nn.Linear(hypo_dim, 1)\n",
    "\n",
    "        self.lin_n = nn.Linear(hypo_dim, hypo_dim)\n",
    "        self.lin_o = nn.Linear(hypo_dim, hypo_dim)\n",
    "\n",
    "        self.lin2_0 = nn.Linear(hypo_dim, hypo_dim)\n",
    "        self.lin2_1 = nn.Linear(hypo_dim, hypo_dim)\n",
    "\n",
    "        self.lin3_0 = nn.Linear(hypo_dim, hypo_dim)\n",
    "        self.lin3_1 = nn.Linear(hypo_dim, hypo_dim)\n",
    "\n",
    "        self.lin4_0 = nn.Linear(hypo_dim, 1)\n",
    "        self.lin4_1 = nn.Linear(hypo_dim, 1)\n",
    "\n",
    "        self.lin2_0_n = nn.Linear(hypo_dim, hypo_dim)\n",
    "        self.lin2_1_n = nn.Linear(hypo_dim, hypo_dim)\n",
    "\n",
    "        self.lin3_0_n = nn.Linear(hypo_dim, hypo_dim)\n",
    "        self.lin3_1_n = nn.Linear(hypo_dim, hypo_dim)\n",
    "\n",
    "        self.lin4_0_n = nn.Linear(hypo_dim, 1)\n",
    "        self.lin4_1_n = nn.Linear(hypo_dim, 1)\n",
    "        if gpu ==1:\n",
    "            self.a=torch.nn.Parameter(torch.tensor(1.)).cuda()#.requires_grad_()\n",
    "            self.b=torch.nn.Parameter(torch.tensor(1.)).cuda()#.requires_grad_()\n",
    "\n",
    "        else:\n",
    "            self.a = torch.nn.Parameter(torch.tensor(1.))#.requires_grad_()\n",
    "            self.b = torch.nn.Parameter(torch.tensor(1.))#.requires_grad_()\n",
    "        \n",
    "        self.num_layers = 1\n",
    "        self.num_directions = 1\n",
    "        self.lstm = nn.LSTM(hypo_dim, hypo_dim, self.num_layers, batch_first=True)\n",
    "        self.lstm_n = nn.LSTM(hypo_dim, hypo_dim, self.num_layers, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "        self.lin_mi_0 = nn.Linear(2 *5* hypo_dim, hypo_dim)\n",
    "        self.lin_mi_1 = nn.Linear(hypo_dim, 1)\n",
    "\n",
    "        for lin in [self.lin1, self.lin1_1, self.lin1_2, self.lin2_0, self.lin2_1, self.lin1_3, self.lin3_0,\n",
    "                    self.lin3_1, self.lin4_0, self.lin4_1,  self.lin_mi_0, self.lin_mi_1, self.lin2_0_n,\n",
    "                    self.lin2_1_n, self.lin3_0_n, self.lin3_1_n, self.lin4_0_n, self.lin4_1_n, self.lin_n,\n",
    "                    self.lin_o]:\n",
    "            nn.init.xavier_uniform_(lin.weight)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        initial = input.view(input.shape)\n",
    "\n",
    "        x = F.relu(self.lin1(initial))\n",
    "        x = F.relu(self.lin1_1(x))\n",
    "        x = F.relu(self.lin1_2(x))\n",
    "        x = F.relu(x)\n",
    "        if net == 'tarnet':\n",
    "            t = self.lin1_3(initial)[:, -1:, :]\n",
    "        else:\n",
    "            t = self.lin1_3(x)[:, -1:, :]\n",
    "\n",
    "        hn_kl = F.relu(self.lin_n(x)) # xn\n",
    "        ho_kl = F.relu(self.lin_o(x)) # xo\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        loss = torch.tensor(0.)\n",
    "        #loss = F.kl_div(hn_kl.softmax(dim=-1).log(), ho_kl.softmax(dim=-1), reduction='mean')\n",
    "        if reg == 'mine':\n",
    "            \n",
    "            hn_kl_mi = hn_kl.view([hn_kl.size(0),-1])\n",
    "            ho_kl_mi = ho_kl.view([ho_kl.size(0),-1])\n",
    "\n",
    "\n",
    "\n",
    "            ######MINE\n",
    "            batch_size = hn_kl_mi.size(0)\n",
    "            tiled_x = torch.cat([hn_kl_mi, hn_kl_mi, ], dim=0)\n",
    "            idx = torch.randperm(batch_size)\n",
    "\n",
    "            shuffled_y = ho_kl_mi[idx]\n",
    "            concat_y = torch.cat([ho_kl_mi, shuffled_y], dim=0)\n",
    "            inputs = torch.cat([tiled_x, concat_y], dim=1)\n",
    "            logits = F.relu(self.lin_mi_0(inputs))\n",
    "            # logits = F.relu(self.lin_mi_1(logits))\n",
    "            logits = self.lin_mi_1(logits)\n",
    "\n",
    "            pred_xy = logits[:batch_size]\n",
    "            pred_x_y = logits[batch_size:]\n",
    "            \n",
    "            loss = np.log2(np.exp(1)) * (torch.abs(torch.mean(pred_xy) - torch.log(torch.mean(torch.exp(pred_x_y)))))\n",
    "            # ######Done MINE\n",
    "        elif reg == 'wasserstein':\n",
    "            ###wasserstein\n",
    "            loss, Mlam = wasserstein(hn_kl, ho_kl, cuda = False)\n",
    "            loss = -loss\n",
    "            ###\n",
    "\n",
    "        \n",
    "        batch_size, seq_len = hn_kl.shape[0], hn_kl.shape[1]\n",
    "        hidden_size = 100\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, hidden_size).cuda()\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, batch_size, hidden_size).cuda()\n",
    "        hn_kl,_ = self.lstm_n(hn_kl, (h_0, c_0))\n",
    "        hn_kl = hn_kl[:, -1:, :] \n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, hidden_size).cuda()\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, batch_size, hidden_size).cuda()\n",
    "        ho_kl,_ = self.lstm(ho_kl, (h_0, c_0))\n",
    "        ho_kl = ho_kl[:, -1:, :] \n",
    "        \n",
    "        # h1, h2 - different group\n",
    "        # xn h1_kl\n",
    "        h_0_n = F.relu(self.lin2_0_n(hn_kl)) # xn\n",
    "        h_1_n = F.relu(self.lin2_1_n(hn_kl)) # xn\n",
    "\n",
    "        h_1_n = F.relu(h_1_n)\n",
    "        h_0_n = F.relu(h_0_n)\n",
    "\n",
    "        h0_p_n = F.relu(self.lin3_0_n(h_0_n))\n",
    "        h1_p_n = F.relu(self.lin3_1_n(h_1_n))\n",
    "\n",
    "        h0_n = self.lin4_0_n(h0_p_n)\n",
    "        h1_n = self.lin4_1_n(h1_p_n)\n",
    "\n",
    "\n",
    "        # h1, h2 - different group\n",
    "        # xn h1_kl\n",
    "        h_0 = F.relu(self.lin2_0(ho_kl)) # xo\n",
    "        h_1 = F.relu(self.lin2_1(ho_kl)) # xo\n",
    "\n",
    "        h_1 = F.relu(h_1)\n",
    "        h_0 = F.relu(h_0)\n",
    "\n",
    "        h0_p = F.relu(self.lin3_0(h_0))\n",
    "        h1_p = F.relu(self.lin3_1(h_1))\n",
    "\n",
    "        h0 = self.lin4_0(h0_p)\n",
    "        h1 = self.lin4_1(h1_p)\n",
    "  \n",
    "        h0_p =torch.add(h0_p, h0_p_n)\n",
    "        h1_p =torch.add(h0_p, h1_p_n)\n",
    "\n",
    "        h0_f = torch.add(self.a*h0, self.b*h0_n)\n",
    "        h1_f = torch.add(self.a*h0, self.b*h1_n)\n",
    "        # print(\"a\", self.a)\n",
    "        # print(\"b\", self.b)\n",
    "\n",
    "        #loss += torch.tensor(1.) - self.a -self.b\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return torch.cat((h0_f, h1_f, t), 2) , loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ite(y0_logit, y1_logit):\n",
    "    y0_pred = torch.sigmoid(y0_logit).float()\n",
    "    y1_pred = torch.sigmoid(y1_logit).float()\n",
    "    return y1_pred - y0_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function helpers\n",
    "def mean_nll(y_logit, y):\n",
    "    return nn.functional.binary_cross_entropy_with_logits(y_logit, y.float())\n",
    "\n",
    "def mean_accuracy(y_logit, y):\n",
    "    preds = (y_logit > 0.).double()\n",
    "    return ((preds - y).abs() < 1e-2).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 1\n",
    "mse = torch.nn.MSELoss()\n",
    "def penalty(y_logit, y):\n",
    "    if gpu ==1:\n",
    "        scale=torch.tensor(1.).cuda().requires_grad_()\n",
    "    else:\n",
    "        scale = torch.tensor(1.).requires_grad_()\n",
    "    loss = mse(y_logit * scale, y)\n",
    "    grad = autograd.grad(loss, [scale], create_graph=True)[0]\n",
    "    res = torch.sum(grad ** 2)\n",
    "    return res\n",
    "\n",
    "\n",
    "def penalty_coco(y_logit, y):\n",
    "    if gpu ==1:\n",
    "        scale = torch.nn.Parameter(torch.normal(1,0.2,[y_logit.shape[1], 1])).cuda().requires_grad_()\n",
    "    else:\n",
    "        scale = torch.nn.Parameter(torch.normal(1,0.2,[y_logit.shape[1], 1])).requires_grad_()\n",
    "    \n",
    "    scale = scale.float()\n",
    "    y_logit = y_logit.float()\n",
    "    y = y.float()\n",
    "    loss = mse(y_logit @ scale, y)\n",
    "    grad = autograd.grad(loss, [scale], create_graph=True)[0]\n",
    "\n",
    "    res = torch.abs(torch.mean((grad**4 *scale)))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(13,100).cuda()\n",
    "lr = 0.001\n",
    "l2_regularizer_weight = 0.001\n",
    "optimizer_adam = optim.Adam(mlp.parameters(), lr=lr)\n",
    "optimizer_sgd = optim.SGD(mlp.parameters(), lr=1e-7, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    #print(epoch)\n",
    "    loss = 0\n",
    "    for i in range(len(all_train_loader)):\n",
    "        for x, y,t in all_train_loader[i]:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            t = t.cuda()\n",
    "\n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "\n",
    "            logits, mi_loss = mlp(x.float())\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            \n",
    "            if gpu == 1:\n",
    "                weight_norm = torch.tensor(0.).cuda()\n",
    "            else:\n",
    "                weight_norm = torch.tensor(0.)\n",
    "            for w in mlp.parameters():\n",
    "                weight_norm += w.norm().pow(2)\n",
    "            \n",
    "            loss = mean_nll(t_logit, t)\n",
    "            loss += mse(y_logit, y)\n",
    "            loss += l2_regularizer_weight * weight_norm\n",
    "            #print(loss)\n",
    "            if not torch.isinf(mi_loss).any() and not torch.isnan(mi_loss).any():\n",
    "                loss += 0.1 * mi_loss\n",
    "            loss += 0.01*penalty_coco(y_logit, y)\n",
    "\n",
    "\n",
    "        \n",
    "            optimizer_adam.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.23508591637230253\n",
      "MSE: 0.09708651222496997\n",
      "RMSE: 0.31158708610109304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.23508591637230253, 0.09708651222496997, 0.31158708610109304)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "# ERM_in_mae =[]\n",
    "# ERM_in_mse = []\n",
    "# ERM_in_rmse = []\n",
    "for i in range(len(all_test_loader)):\n",
    "        for x, y,t in all_test_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            \n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "report_res(y_pred, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.22443563019572813\n",
      "MSE: 0.09300794632097215\n",
      "RMSE: 0.30497204186772947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22443563019572813, 0.09300794632097215, 0.30497204186772947)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "# ERM_in_mae =[]\n",
    "# ERM_in_mse = []\n",
    "# ERM_in_rmse = []\n",
    "for i in range(len(all_test_loader)):\n",
    "        for x, y,t in all_test_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            \n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "report_res(y_pred, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.19079634337671103\n",
      "MSE: 0.05646493558114684\n",
      "RMSE: 0.2376235164733214\n",
      "Results \n",
      "MAE: 0.2258738535195262\n",
      "MSE: 0.08425541642115372\n",
      "RMSE: 0.2902678356641564\n",
      "Results \n",
      "MAE: 0.20748424019971112\n",
      "MSE: 0.08890331848637284\n",
      "RMSE: 0.2981665951886174\n",
      "Results \n",
      "MAE: 0.22376905021378732\n",
      "MSE: 0.09310422843034435\n",
      "RMSE: 0.3051298550295339\n",
      "Results \n",
      "MAE: 0.21323903587967846\n",
      "MSE: 0.08515539679746284\n",
      "RMSE: 0.2918139763573068\n",
      "Results \n",
      "MAE: 0.1789755952688881\n",
      "MSE: 0.056674143043701004\n",
      "RMSE: 0.23806331729962305\n",
      "Results \n",
      "MAE: 0.2139190831844433\n",
      "MSE: 0.08651345740936185\n",
      "RMSE: 0.2941317007895644\n"
     ]
    }
   ],
   "source": [
    "Pro_out_mae =[]\n",
    "Pro_out_mse = []\n",
    "Pro_out_rmse = []\n",
    "for i in range(len(all_test_in_loader)):\n",
    "    y_all = []\n",
    "    y_pred = []\n",
    "    for x, y,t in all_test_in_loader[i]:\n",
    "        x = x.cuda()\n",
    "        #y = y.cuda()\n",
    "        t = t.cuda()\n",
    "\n",
    "        x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "        t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "        y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "        #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "        logits, mi = mlp(x.float())\n",
    "        #print(logit.shape)\n",
    "        y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "        y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "        t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "        y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "        if len(y_all) == 0:\n",
    "            y_all = np.array(y).reshape(-1)\n",
    "            y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "        else:\n",
    "            y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "            y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "    mae_f, mse_f, rmse_f = report_res(y_pred, y_all)\n",
    "    Pro_out_mse.append(mse_f)\n",
    "    Pro_out_mae.append(mae_f)\n",
    "    Pro_out_rmse.append(rmse_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.21086974615804555\n",
      "MSE: 0.07931272167603294\n",
      "RMSE: 0.2816251438988233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21086974615804555, 0.07931272167603294, 0.2816251438988233)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "for i in range(len(all_test_in_loader)):\n",
    "        for x, y,t in all_test_in_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            \n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "report_res(y_pred, y_all)\n",
    "            #print(y_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.19041473082269966\n",
      "MSE: 0.05623527870588155\n",
      "RMSE: 0.23713978726877855\n",
      "Results \n",
      "MAE: 0.225091332103704\n",
      "MSE: 0.08417954671463351\n",
      "RMSE: 0.29013711709230433\n",
      "Results \n",
      "MAE: 0.20757464079087518\n",
      "MSE: 0.08946155844498142\n",
      "RMSE: 0.2991012511591709\n",
      "Results \n",
      "MAE: 0.22546348503268024\n",
      "MSE: 0.094589867302112\n",
      "RMSE: 0.307554657422241\n",
      "Results \n",
      "MAE: 0.21286762215649638\n",
      "MSE: 0.08496595679322183\n",
      "RMSE: 0.2914892052773513\n",
      "Results \n",
      "MAE: 0.18051344175248626\n",
      "MSE: 0.05732322674349212\n",
      "RMSE: 0.23942269471270286\n",
      "Results \n",
      "MAE: 0.21423055054732532\n",
      "MSE: 0.0855584269765025\n",
      "RMSE: 0.29250372130368274\n"
     ]
    }
   ],
   "source": [
    "Pro_in_mae =[]\n",
    "Pro_in_mse = []\n",
    "Pro_in_rmse = []\n",
    "for i in range(len(all_test_in_loader)):\n",
    "    y_all = []\n",
    "    y_pred = []\n",
    "    for x, y,t in all_test_in_loader[i]:\n",
    "        x = x.cuda()\n",
    "        #y = y.cuda()\n",
    "        t = t.cuda()\n",
    "\n",
    "        x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "        t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "        y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "        #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "        logits, mi = mlp(x.float())\n",
    "        #print(logit.shape)\n",
    "        y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "        y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "        t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "        y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "        if len(y_all) == 0:\n",
    "            y_all = np.array(y).reshape(-1)\n",
    "            y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "        else:\n",
    "            y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "            y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "    mae_f, mse_f, rmse_f = report_res(y_pred, y_all)\n",
    "    Pro_in_mse.append(mse_f)\n",
    "    Pro_in_mae.append(mae_f)\n",
    "    Pro_in_rmse.append(rmse_f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_in_loader = []\n",
    "for key in train_envs.keys():\n",
    "    dataset_train = Dataset_in_test(train_envs[key]['previous_covariates'], train_envs[key]['outcomes'],train_envs[key]['previous_treatments'], T=5)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=4, shuffle=True)  \n",
    "    all_test_in_loader.append(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "for i in range(len(all_test_in_loader)):\n",
    "        for x, y,t in all_test_in_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi_loss = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.22874074805136743\n",
      "MSE: 0.08574023929682352\n",
      "RMSE: 0.29281434271022916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22874074805136743, 0.08574023929682352, 0.29281434271022916)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_res(y_pred, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(13,100).cuda()\n",
    "lr = 0.001\n",
    "l2_regularizer_weight = 0.001\n",
    "optimizer_adam = optim.Adam(mlp.parameters(), lr=lr)\n",
    "optimizer_sgd = optim.SGD(mlp.parameters(), lr=1e-7, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    #print(epoch)\n",
    "    loss = 0\n",
    "    for i in range(len(all_train_loader)):\n",
    "        for x, y,t in all_train_loader[i]:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            t = t.cuda()\n",
    "\n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "\n",
    "            logits, mi_loss = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "#             print(t.shape)\n",
    "#             print(y1_logit.shape)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            \n",
    "            if gpu == 1:\n",
    "                weight_norm = torch.tensor(0.).cuda()\n",
    "            else:\n",
    "                weight_norm = torch.tensor(0.)\n",
    "            for w in mlp.parameters():\n",
    "                weight_norm += w.norm().pow(2)\n",
    "            \n",
    "            loss += mean_nll(t_logit, t)\n",
    "            loss += mse(y_logit, y)\n",
    "            loss += l2_regularizer_weight * weight_norm\n",
    "            #print(loss)\n",
    "            if not torch.isinf(mi_loss).any() and not torch.isnan(mi_loss).any():\n",
    "                #print(mi_loss)\n",
    "                loss = 0.1 * mi_loss\n",
    "            loss += 0.01*penalty_coco(y_logit, y)\n",
    "        #print(i, ite(y0_logit, y1_logit).mean())\n",
    "        #print(i, mse(y_logit, y).mean())\n",
    "\n",
    "\n",
    "        \n",
    "    optimizer_adam.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.29308792773010195\n",
      "MSE: 0.19206802748397342\n",
      "RMSE: 0.43825566452012166\n"
     ]
    }
   ],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "for i in range(len(all_test_loader)):\n",
    "        for x, y,t in all_test_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            \n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi_loss = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "\n",
    "report_res(y_pred, y_all)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results \n",
      "MAE: 0.2513970825176546\n",
      "MSE: 0.14099167850264693\n",
      "RMSE: 0.37548858638132654\n"
     ]
    }
   ],
   "source": [
    "y_all = []\n",
    "y_pred = []\n",
    "for i in range(len(all_test_in_loader)):\n",
    "        for x, y,t in all_test_in_loader[i]:\n",
    "            x = x.cuda()\n",
    "            #y = y.cuda()\n",
    "            t = t.cuda()\n",
    "            x = x.view([x.size()[0]*x.size()[1],x.size()[2], -1])\n",
    "            t = t.view([t.size()[0]*t.size()[1], -1,1])\n",
    "            y = y.view([y.size()[0]*y.size()[1], -1,1])\n",
    "\n",
    "            #x = x.view([x.size()[0], x.size()[1], -1])\n",
    "            logits, mi_loss = mlp(x.float())\n",
    "            #print(logit.shape)\n",
    "            y0_logit = logits[:,:, 0].unsqueeze(2)\n",
    "            y1_logit = logits[:,:, 1].unsqueeze(2)\n",
    "            t_logit = logits[:,:, 2].unsqueeze(2)\n",
    "            y_logit = t * y1_logit + (1 - t) * y0_logit\n",
    "            if len(y_all) == 0:\n",
    "                y_all = np.array(y).reshape(-1)\n",
    "                y_pred = np.array(y_logit.detach().cpu()).reshape(-1)\n",
    "            else:\n",
    "                y_all = np.hstack((y_all, np.array(y).reshape(-1)))\n",
    "                y_pred = np.hstack((y_pred, np.array(y_logit.detach().cpu()).reshape(-1)))\n",
    "                \n",
    "report_res(y_pred, y_all)  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ode",
   "language": "python",
   "name": "ode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
